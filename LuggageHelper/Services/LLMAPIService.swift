import Foundation
import Combine
import os.log

/// LLM APIæœåŠ¡
/// æä¾›ä¸å„ç§LLM APIçš„å®Œæ•´äº¤äº’åŠŸèƒ½ï¼Œæ”¯æŒOpenAIå’ŒAnthropicæ ¼å¼
final class LLMAPIService: ObservableObject {
    
    // MARK: - å•ä¾‹æ¨¡å¼
    
    /// å…±äº«å®ä¾‹
    static let shared = LLMAPIService()
    
    /// å–æ¶ˆè®¢é˜…é›†åˆ
    private var cancellables = Set<AnyCancellable>()
    
    /// ç§æœ‰åˆå§‹åŒ–
    private init() {
        // ç›‘å¬é…ç½®å˜æ›´
        LLMConfigurationManager.shared.configurationChanged
            .sink { [weak self] in
                self?.syncConfiguration()
            }
            .store(in: &cancellables)
        
        // ç«‹å³åŒæ­¥é…ç½®
        syncConfiguration()
    }
    
    // MARK: - æ—¥å¿—é…ç½®
    
    /// æ—¥å¿—è®°å½•å™¨
    internal let logger = Logger(subsystem: "com.luggagehelper.api", category: "LLMAPI")
    
    /// æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—
    internal let enableDetailedLogging = true
    
    // MARK: - å¯è§‚å¯Ÿå±æ€§
    
    /// å½“å‰APIé…ç½®
    @Published var currentConfig: LLMServiceConfig?
    
    /// æ˜¯å¦æ­£åœ¨åŠ è½½
    @Published var isLoading = false
    
    /// é”™è¯¯æ¶ˆæ¯
    @Published var errorMessage: String? = nil
    
    // MARK: - ç½‘ç»œé…ç½®
    
    /// URLSessioné…ç½®
    private let session: URLSession = {
        let config = URLSessionConfiguration.default
        config.timeoutIntervalForRequest = 60  // å¢åŠ åˆ°60ç§’
        config.timeoutIntervalForResource = 120 // å¢åŠ åˆ°120ç§’
        return URLSession(configuration: config)
    }()
    
    /// JSONç¼–ç å™¨
    private let encoder: JSONEncoder = {
        let encoder = JSONEncoder()
        encoder.keyEncodingStrategy = .convertToSnakeCase
        encoder.outputFormatting = .prettyPrinted
        return encoder
    }()
    
    /// JSONè§£ç å™¨
    private let decoder: JSONDecoder = {
        let decoder = JSONDecoder()
        decoder.keyDecodingStrategy = .convertFromSnakeCase
        return decoder
    }()
    
    // MARK: - æä¾›å•†ç±»å‹
    
    /// LLMæä¾›å•†ç±»å‹
    enum LLMProviderType: String, CaseIterable, Codable {
        case openai = "openai"      // æ”¯æŒ OpenAI æ ¼å¼ (OpenAI, ç¡…åŸºæµåŠ¨, æ™ºè°±AIç­‰)
        case anthropic = "anthropic" // æ”¯æŒ Anthropic æ ¼å¼
        
        var displayName: String {
            switch self {
            case .openai: return "OpenAI æ ¼å¼"
            case .anthropic: return "Anthropic æ ¼å¼"
            }
        }
        
        var defaultEndpoint: String {
            switch self {
            case .openai: return "/v1/chat/completions"
            case .anthropic: return "/v1/messages"
            }
        }
        
        var description: String {
            switch self {
            case .openai: return "å…¼å®¹ OpenAIã€ç¡…åŸºæµåŠ¨ã€æ™ºè°±AI ç­‰æœåŠ¡å•†"
            case .anthropic: return "å…¼å®¹ Anthropic Claude API"
            }
        }
    }
    
    // MARK: - é…ç½®ç»“æ„
    
    /// LLMæœåŠ¡é…ç½®
    struct LLMServiceConfig: Codable {
        let providerType: LLMProviderType
        let baseURL: String
        let apiKey: String
        let model: String
        let maxTokens: Int?
        let temperature: Double?
        let topP: Double?
        let topK: Int?
        let frequencyPenalty: Double?
        let stop: [String]?
        
        init(providerType: LLMProviderType = .openai,
             baseURL: String,
             apiKey: String,
             model: String,
             maxTokens: Int? = 2048,
             temperature: Double? = 0.7,
             topP: Double? = 0.9,
             topK: Int? = 50,
             frequencyPenalty: Double? = 0.0,
             stop: [String]? = nil) {
            self.providerType = providerType
            self.baseURL = baseURL
            self.apiKey = apiKey
            self.model = model
            self.maxTokens = maxTokens
            self.temperature = temperature
            self.topP = topP
            self.topK = topK
            self.frequencyPenalty = frequencyPenalty
            self.stop = stop
        }
        
        /// æ£€æŸ¥é…ç½®æ˜¯å¦æœ‰æ•ˆ
        func isValid() -> Bool {
            return !baseURL.isEmpty && !apiKey.isEmpty && !model.isEmpty
        }
        
        /// è·å–å®Œæ•´çš„APIç«¯ç‚¹URL
        var fullEndpointURL: String {
            let cleanBaseURL = baseURL.trimmingCharacters(in: CharacterSet(charactersIn: "/"))
            return cleanBaseURL + providerType.defaultEndpoint
        }
    }
    
    // MARK: - é”™è¯¯ç±»å‹
    
    /// APIé”™è¯¯ç±»å‹
    enum APIError: LocalizedError {
        case invalidURL
        case invalidResponse
        case serverError(statusCode: Int, message: String?)
        case decodingError(Error)
        case encodingError(Error)
        case networkError(Error)
        case configurationError(String)
        case rateLimitExceeded
        case authenticationFailed
        case insufficientData
        case unsupportedProvider(LLMProviderType)
        
        var errorDescription: String? {
            switch self {
            case .invalidURL:
                return "æ— æ•ˆçš„APIåœ°å€"
            case .invalidResponse:
                return "æœåŠ¡å™¨å“åº”æ— æ•ˆ"
            case .serverError(let statusCode, let message):
                return "æœåŠ¡å™¨é”™è¯¯ (çŠ¶æ€ç : \(statusCode)) - \(message ?? "æœªçŸ¥é”™è¯¯")"
            case .decodingError(let error):
                return "æ•°æ®è§£æå¤±è´¥: \(error.localizedDescription)"
            case .encodingError(let error):
                return "æ•°æ®ç¼–ç å¤±è´¥: \(error.localizedDescription)"
            case .networkError(let error):
                return "ç½‘ç»œé”™è¯¯: \(error.localizedDescription)"
            case .configurationError(let message):
                return "é…ç½®é”™è¯¯: \(message)"
            case .rateLimitExceeded:
                return "è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åå†è¯•"
            case .authenticationFailed:
                return "è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥"
            case .insufficientData:
                return "æä¾›çš„æ•°æ®ä¸è¶³"
            case .unsupportedProvider(let provider):
                return "ä¸æ”¯æŒçš„æä¾›å•†ç±»å‹: \(provider.displayName)"
            }
        }
    }

    // MARK: - è¯·æ±‚æ¨¡å‹
    
    /// èŠå¤©æ¶ˆæ¯æ¨¡å‹
    struct ChatMessage: Codable {
        let role: String
        let content: String
        
        init(role: String, content: String) {
            self.role = role
            self.content = content
        }
        
        static func system(_ content: String) -> ChatMessage {
            return ChatMessage(role: "system", content: content)
        }
        
        static func user(_ content: String) -> ChatMessage {
            return ChatMessage(role: "user", content: content)
        }
        
        static func assistant(_ content: String) -> ChatMessage {
            return ChatMessage(role: "assistant", content: content)
        }
    }
    
    /// èŠå¤©å®Œæˆè¯·æ±‚æ¨¡å‹
    struct ChatCompletionRequest: Codable {
        let model: String
        let messages: [ChatMessage]
        let maxTokens: Int?
        let temperature: Double?
        let topP: Double?
        let stream: Bool
        let responseFormat: ResponseFormat?
        let topK: Int?
        let frequencyPenalty: Double?
        let stop: [String]?
        
        struct ResponseFormat: Codable {
            let type: String
        }
        
        init(model: String, messages: [ChatMessage], maxTokens: Int? = nil, 
             temperature: Double? = nil, topP: Double? = nil, stream: Bool = false,
             responseFormat: ResponseFormat? = nil, topK: Int? = nil, 
             frequencyPenalty: Double? = nil, stop: [String]? = nil) {
            self.model = model
            self.messages = messages
            self.maxTokens = maxTokens
            self.temperature = temperature
            self.topP = topP
            self.stream = stream
            self.responseFormat = responseFormat
            self.topK = topK
            self.frequencyPenalty = frequencyPenalty
            self.stop = stop
        }
    }
    
    /// èŠå¤©å®Œæˆå“åº”æ¨¡å‹
    struct ChatCompletionResponse: Codable {
        let id: String
        let object: String
        let created: Int
        let model: String
        let choices: [Choice]
        let usage: Usage?
        
        struct Choice: Codable {
            let index: Int
            let message: ChatMessage
            let finishReason: String?
        }
        
        struct Usage: Codable {
            let promptTokens: Int
            let completionTokens: Int
            let totalTokens: Int
        }
    }
    
    /// æµå¼å“åº”æ•°æ®æ¨¡å‹
    struct StreamingResponse: Codable {
        let id: String
        let object: String
        let created: Int
        let model: String
        let choices: [StreamChoice]
        
        struct StreamChoice: Codable {
            let index: Int
            let delta: Delta
            let finishReason: String?
            
            struct Delta: Codable {
                let role: String?
                let content: String?
            }
        }
    }
    
    // MARK: - é…ç½®æ£€æŸ¥æ–¹æ³•
    
    /// æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²æ­£ç¡®é…ç½®
    func isConfigured() -> Bool {
        guard let validConfig = currentConfig else { return false }
        return validConfig.isValid()
    }
    
    // MARK: - ä¸»è¦APIæ–¹æ³•
    
    /// å‘é€èŠå¤©å®Œæˆè¯·æ±‚ï¼ˆåŒæ­¥ï¼‰
    /// - Parameters:
    ///   - messages: æ¶ˆæ¯åˆ—è¡¨
    ///   - config: APIé…ç½®ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
    /// - Returns: å“åº”ç»“æœ
    func sendChatCompletion(
        messages: [ChatMessage],
        config: LLMServiceConfig? = nil
    ) async throws -> ChatCompletionResponse {
        let config = config ?? currentConfig ?? LLMConfigurationManager.shared.currentConfig
        
        guard config.isValid() else {
            throw APIError.configurationError("LLM APIé…ç½®æ— æ•ˆ")
        }
        
        let request = ChatCompletionRequest(
            model: config.model,
            messages: messages,
            maxTokens: config.maxTokens,
            temperature: config.temperature,
            topP: config.topP,
            stream: false,
            responseFormat: nil,
            topK: config.topK,
            frequencyPenalty: config.frequencyPenalty,
            stop: config.stop
        )
        
        return try await performRequest(request, config: config)
    }
    
    /// å‘é€æµå¼èŠå¤©å®Œæˆè¯·æ±‚
    /// - Parameters:
    ///   - messages: æ¶ˆæ¯åˆ—è¡¨
    ///   - config: APIé…ç½®ï¼ˆå¯é€‰ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
    /// - Returns: æµå¼å“åº”å‘å¸ƒè€…
    func sendStreamingChatCompletion(
        messages: [ChatMessage],
        config: LLMServiceConfig? = nil
    ) -> AnyPublisher<String, Error> {
        let config = config ?? currentConfig ?? LLMConfigurationManager.shared.currentConfig
        
        guard config.isValid() else {
            return Fail(error: APIError.configurationError("LLM APIé…ç½®æ— æ•ˆ"))
                .eraseToAnyPublisher()
        }
        
        let request = ChatCompletionRequest(
            model: config.model,
            messages: messages,
            maxTokens: config.maxTokens,
            temperature: config.temperature,
            topP: config.topP,
            stream: true,
            responseFormat: nil,
            topK: config.topK,
            frequencyPenalty: config.frequencyPenalty,
            stop: config.stop
        )
        
        return performStreamingRequest(request, config: config)
    }
    
    /// æµ‹è¯•APIè¿æ¥
    /// - Parameter config: APIé…ç½®ï¼ˆå¯é€‰ï¼‰
    /// - Returns: æµ‹è¯•ç»“æœ
    func testConnection(config: LLMServiceConfig? = nil) async throws -> String {
        let config = config ?? currentConfig ?? LLMConfigurationManager.shared.currentConfig
        
        guard config.isValid() else {
            throw APIError.configurationError("LLM APIé…ç½®æ— æ•ˆ")
        }
        
        let testMessages = [
            ChatMessage.system("ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹ã€‚"),
            ChatMessage.user("è¯·å›å¤'è¿æ¥æµ‹è¯•æˆåŠŸ'")
        ]
        
        let request = ChatCompletionRequest(
            model: config.model,
            messages: testMessages,
            maxTokens: 50,
            temperature: 0.1,
            topP: 0.9,
            stream: false,
            responseFormat: nil,
            topK: 50,
            frequencyPenalty: 0.0,
            stop: nil
        )
        
        do {
            let response = try await performRequest(request, config: config)
            
            if let content = response.choices.first?.message.content {
                logger.info("LLM APIè¿æ¥æµ‹è¯•æˆåŠŸ")
                return content
            } else {
                throw APIError.invalidResponse
            }
        } catch {
            logger.error("LLM APIè¿æ¥æµ‹è¯•å¤±è´¥: \(error.localizedDescription)")
            throw error
        }
    }
    
    // MARK: - ç§æœ‰æ–¹æ³•
    
    /// æ‰§è¡ŒAPIè¯·æ±‚
    internal func performRequest(
        _ request: ChatCompletionRequest,
        config: LLMServiceConfig
    ) async throws -> ChatCompletionResponse {
        
        // æ ¹æ®æä¾›å•†ç±»å‹é€‰æ‹©é€‚é…å™¨
        switch config.providerType {
        case .openai:
            return try await performOpenAIRequest(request, config: config)
        case .anthropic:
            return try await performAnthropicRequest(request, config: config)
        }
    }
    
    /// æ‰§è¡ŒOpenAIæ ¼å¼è¯·æ±‚
    private func performOpenAIRequest(
        _ request: ChatCompletionRequest,
        config: LLMServiceConfig
    ) async throws -> ChatCompletionResponse {
        
        guard let url = URL(string: config.fullEndpointURL) else {
            throw APIError.invalidURL
        }
        
        var urlRequest = URLRequest(url: url)
        urlRequest.httpMethod = "POST"
        urlRequest.setValue("application/json", forHTTPHeaderField: "Content-Type")
        urlRequest.setValue("Bearer \(config.apiKey)", forHTTPHeaderField: "Authorization")
        
        do {
            urlRequest.httpBody = try encoder.encode(request)
        } catch {
            throw APIError.encodingError(error)
        }
        
        if enableDetailedLogging {
            logger.info("å‘é€OpenAIæ ¼å¼APIè¯·æ±‚åˆ°: \(url.absoluteString)")
            logger.debug("è¯·æ±‚ä½“: \(String(data: urlRequest.httpBody ?? Data(), encoding: .utf8) ?? "æ— æ³•è§£æ")")
        }
        
        return try await executeRequest(urlRequest)
    }
    
    /// æ‰§è¡ŒAnthropicæ ¼å¼è¯·æ±‚
    private func performAnthropicRequest(
        _ request: ChatCompletionRequest,
        config: LLMServiceConfig
    ) async throws -> ChatCompletionResponse {
        
        guard let url = URL(string: config.fullEndpointURL) else {
            throw APIError.invalidURL
        }
        
        // è½¬æ¢ä¸ºAnthropicæ ¼å¼
        let anthropicRequest = convertToAnthropicFormat(request)
        
        var urlRequest = URLRequest(url: url)
        urlRequest.httpMethod = "POST"
        urlRequest.setValue("application/json", forHTTPHeaderField: "Content-Type")
        urlRequest.setValue(config.apiKey, forHTTPHeaderField: "x-api-key")
        urlRequest.setValue("2023-06-01", forHTTPHeaderField: "anthropic-version")
        
        do {
            urlRequest.httpBody = try JSONEncoder().encode(anthropicRequest)
        } catch {
            throw APIError.encodingError(error)
        }
        
        if enableDetailedLogging {
            logger.info("å‘é€Anthropicæ ¼å¼APIè¯·æ±‚åˆ°: \(url.absoluteString)")
            logger.debug("è¯·æ±‚ä½“: \(String(data: urlRequest.httpBody ?? Data(), encoding: .utf8) ?? "æ— æ³•è§£æ")")
        }
        
        // æ‰§è¡Œè¯·æ±‚å¹¶è½¬æ¢å“åº”
        let anthropicResponse = try await executeAnthropicRequest(urlRequest)
        return convertFromAnthropicFormat(anthropicResponse)
    }
    
    /// æ‰§è¡Œé€šç”¨HTTPè¯·æ±‚
    private func executeRequest(_ urlRequest: URLRequest) async throws -> ChatCompletionResponse {
        do {
            let (data, response) = try await session.data(for: urlRequest)
            
            guard let httpResponse = response as? HTTPURLResponse else {
                throw APIError.invalidResponse
            }
            
            if enableDetailedLogging {
                logger.info("æ”¶åˆ°å“åº”ï¼ŒçŠ¶æ€ç : \(httpResponse.statusCode)")
                logger.debug("å“åº”ä½“: \(String(data: data, encoding: .utf8) ?? "æ— æ³•è§£æ")")
            }
            
            // æ£€æŸ¥HTTPçŠ¶æ€ç 
            guard 200...299 ~= httpResponse.statusCode else {
                let errorMessage = String(data: data, encoding: .utf8)
                throw APIError.serverError(statusCode: httpResponse.statusCode, message: errorMessage)
            }
            
            // è§£æå“åº”
            do {
                let chatResponse = try decoder.decode(ChatCompletionResponse.self, from: data)
                return chatResponse
            } catch {
                logger.error("å“åº”è§£æå¤±è´¥: \(error)")
                throw APIError.decodingError(error)
            }
            
        } catch {
            if error is APIError {
                throw error
            } else {
                throw APIError.networkError(error)
            }
        }
    }
    
    /// æ‰§è¡Œæµå¼è¯·æ±‚
    private func performStreamingRequest(
        _ request: ChatCompletionRequest,
        config: LLMServiceConfig,
        onReceive: @escaping (String) -> Void = { _ in }
    ) -> AnyPublisher<String, Error> {
        
        // ç›®å‰åªæ”¯æŒOpenAIæ ¼å¼çš„æµå¼è¯·æ±‚
        guard config.providerType == .openai else {
            return Fail(error: APIError.unsupportedProvider(config.providerType))
                .eraseToAnyPublisher()
        }
        
        guard let url = URL(string: config.fullEndpointURL) else {
            return Fail(error: APIError.invalidURL)
                .eraseToAnyPublisher()
        }
        
        var urlRequest = URLRequest(url: url)
        urlRequest.httpMethod = "POST"
        urlRequest.setValue("application/json", forHTTPHeaderField: "Content-Type")
        urlRequest.setValue("Bearer \(config.apiKey)", forHTTPHeaderField: "Authorization")
        urlRequest.setValue("text/event-stream", forHTTPHeaderField: "Accept")
        
        do {
            urlRequest.httpBody = try encoder.encode(request)
        } catch {
            return Fail(error: APIError.encodingError(error))
                .eraseToAnyPublisher()
        }
        
        return session.dataTaskPublisher(for: urlRequest)
            .tryMap { data, response -> Data in
                guard let httpResponse = response as? HTTPURLResponse else {
                    throw APIError.invalidResponse
                }
                
                guard 200...299 ~= httpResponse.statusCode else {
                    let errorMessage = String(data: data, encoding: .utf8)
                    throw APIError.serverError(statusCode: httpResponse.statusCode, message: errorMessage)
                }
                
                return data
            }
            .flatMap { data -> AnyPublisher<String, Error> in
                let dataString = String(data: data, encoding: .utf8) ?? ""
                let lines = dataString.components(separatedBy: .newlines)
                
                return Publishers.Sequence(sequence: lines)
                    .compactMap { line -> String? in
                        // å¤„ç†SSEæ ¼å¼
                        if line.hasPrefix("data: ") {
                            let jsonString = String(line.dropFirst(6))
                            
                            if jsonString == "[DONE]" {
                                return nil
                            }
                            
                            guard let jsonData = jsonString.data(using: .utf8) else {
                                return nil
                            }
                            
                            do {
                                let streamResponse = try JSONDecoder().decode(StreamingResponse.self, from: jsonData)
                                return streamResponse.choices.first?.delta.content
                            } catch {
                                return nil
                            }
                        }
                        return nil
                    }
                    .compactMap { $0 }
                    .eraseToAnyPublisher()
            }
            .eraseToAnyPublisher()
    }
}

// MARK: - Anthropicæ ¼å¼è½¬æ¢æ‰©å±•

extension LLMAPIService {
    
    /// Anthropicè¯·æ±‚æ ¼å¼
    private struct AnthropicRequest: Codable {
        let model: String
        let max_tokens: Int
        let messages: [AnthropicMessage]
        let system: String?
        let temperature: Double?
        let top_p: Double?
        let top_k: Int?
        let stop_sequences: [String]?
    }
    
    /// Anthropicæ¶ˆæ¯æ ¼å¼
    private struct AnthropicMessage: Codable {
        let role: String
        let content: String
    }
    
    /// Anthropicå“åº”æ ¼å¼
    private struct AnthropicResponse: Codable {
        let id: String
        let type: String
        let role: String
        let content: [AnthropicContent]
        let model: String
        let stop_reason: String?
        let stop_sequence: String?
        let usage: AnthropicUsage
        
        struct AnthropicContent: Codable {
            let type: String
            let text: String
        }
        
        struct AnthropicUsage: Codable {
            let input_tokens: Int
            let output_tokens: Int
        }
    }
    
    /// è½¬æ¢ä¸ºAnthropicæ ¼å¼
    private func convertToAnthropicFormat(_ request: ChatCompletionRequest) -> AnthropicRequest {
        var systemMessage: String?
        var messages: [AnthropicMessage] = []
        
        for message in request.messages {
            if message.role == "system" {
                systemMessage = message.content
            } else {
                messages.append(AnthropicMessage(role: message.role, content: message.content))
            }
        }
        
        return AnthropicRequest(
            model: request.model,
            max_tokens: request.maxTokens ?? 2048,
            messages: messages,
            system: systemMessage,
            temperature: request.temperature,
            top_p: request.topP,
            top_k: request.topK,
            stop_sequences: request.stop
        )
    }
    
    /// æ‰§è¡ŒAnthropicè¯·æ±‚
    private func executeAnthropicRequest(_ urlRequest: URLRequest) async throws -> AnthropicResponse {
        do {
            let (data, response) = try await session.data(for: urlRequest)
            
            guard let httpResponse = response as? HTTPURLResponse else {
                throw APIError.invalidResponse
            }
            
            if enableDetailedLogging {
                logger.info("æ”¶åˆ°Anthropicå“åº”ï¼ŒçŠ¶æ€ç : \(httpResponse.statusCode)")
                logger.debug("å“åº”ä½“: \(String(data: data, encoding: .utf8) ?? "æ— æ³•è§£æ")")
            }
            
            guard 200...299 ~= httpResponse.statusCode else {
                let errorMessage = String(data: data, encoding: .utf8)
                throw APIError.serverError(statusCode: httpResponse.statusCode, message: errorMessage)
            }
            
            do {
                let anthropicResponse = try JSONDecoder().decode(AnthropicResponse.self, from: data)
                return anthropicResponse
            } catch {
                logger.error("Anthropicå“åº”è§£æå¤±è´¥: \(error)")
                throw APIError.decodingError(error)
            }
            
        } catch {
            if error is APIError {
                throw error
            } else {
                throw APIError.networkError(error)
            }
        }
    }
    
    /// ä»Anthropicæ ¼å¼è½¬æ¢
    private func convertFromAnthropicFormat(_ response: AnthropicResponse) -> ChatCompletionResponse {
        let content = response.content.first?.text ?? ""
        
        let choice = ChatCompletionResponse.Choice(
            index: 0,
            message: ChatMessage(role: "assistant", content: content),
            finishReason: response.stop_reason
        )
        
        let usage = ChatCompletionResponse.Usage(
            promptTokens: response.usage.input_tokens,
            completionTokens: response.usage.output_tokens,
            totalTokens: response.usage.input_tokens + response.usage.output_tokens
        )
        
        return ChatCompletionResponse(
            id: response.id,
            object: "chat.completion",
            created: Int(Date().timeIntervalSince1970),
            model: response.model,
            choices: [choice],
            usage: usage
        )
    }


    // MARK: - Configuration Sync

    /// åŒæ­¥é…ç½®
    func syncConfiguration() {
        self.currentConfig = LLMConfigurationManager.shared.currentConfig
        print("ğŸ”„ LLMAPIServiceé…ç½®å·²åŒæ­¥:")
        print("   - é…ç½®æœ‰æ•ˆæ€§: \(currentConfig?.isValid() ?? false)")
        print("   - baseURL: \(currentConfig?.baseURL ?? "æœªè®¾ç½®")")
        print("   - model: \(currentConfig?.model ?? "æœªè®¾ç½®")")
    }

    /// ç¡®ä¿é…ç½®åŒæ­¥
    /// - Returns: å½“å‰æœ‰æ•ˆçš„é…ç½®
    func ensureConfigurationSync() -> LLMServiceConfig {
        let managerConfig = LLMConfigurationManager.shared.currentConfig
        if currentConfig == nil || !currentConfig!.isValid() {
            currentConfig = managerConfig
            print("âš ï¸ æ£€æµ‹åˆ°é…ç½®ä¸åŒæ­¥ï¼Œå·²é‡æ–°åŒæ­¥")
        }
        return currentConfig ?? managerConfig
    }
}